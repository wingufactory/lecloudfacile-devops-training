# Intro
Apache airflow est un outil data engineer qui a vu le jour en 2025 echez airbnb. A cette epoque, ce dernier faisait face à une croissance fulgurante et des volumes de donnees massifs à traiter. C'est d'ailleurs de là que les ingenieurs airbnb ont pu creer l'outil intitulé `airflow` jusqu'en 2016, ou l'outil rejoint  l’incubateur officiel de la Fondation Apache.


## Qu'est-ce que Apache Airflow ? A quoi il sert ?
Apache Airflow vise à permettre aux équipes de créer, planifier et de surveiller les pipelines de données en lot et d’itérer. Il s’agit d’une solution totalement open source, très utile pour l’architecture et l’orchestration de pipelines de données complexes et le lancer de tâches.

Apache Airflow est une plateforme :
- Dynamique: tout ce qui peut être fait avec le code Python peut être fait sur Airflow
- extensible: grâce à de nombreux plugins permettant l’interaction avec la plupart des systèmes externes les plus communs. Il est aussi possible de créer de nouveaux plugins pour répondre à des besoins spécifiques.
- élastique: airflow est utilisé pour exécuter des milliers de tâches différentes chaque jour.

## Quels sont les différents composants d'Airflow ?

![](img/Screenshot%202024-09-15%20at%2014.28.08.png)

L’architecture Airflow repose sur plusieurs éléments dont:

### DAG (Directed Acyclic Graph)

Dans Airflow, les pipelines sont représentés sous forme de DAG (Directed Acyclic Graph) défini en Python. 

Un graphe est une structure composée d’objets (nœuds) dans laquelle certaines paires d’objets sont en relation.

Les DAGs sont 
- `Directed` (orientés), cela veut dire que les arrêtes du graphes sont orientées, et qu’elles représentent donc des liens unidirectionnels.
- `Acycliques`, C’est à dire  qu’un nœud B en aval d’un nœud A ne peut pas également être en amont du nœud A. Ceci permet de s’assurer que les pipelines ne comportent pas de boucles infinies.

### Les tâches (Tasks)

Chaque noeud d’un DAG représente une tâche. Il s’agit d’une représentation d’une suite de tâches à effectuer, qui constitue une pipeline.

### Operateurs

Les opérateurs sont les blocs de construction de la plateforme Airflow. Ils permettent de déterminer les travaux effectués. Il peut s’agir d’une tâche individuelle (noeud d’un DAG), définissant comment la tâche sera exécutée.

`Le DAG permet de s’assurer que les opérateurs soient programmés et exécutés dans un ordre précis, tandis que les opérateurs définissent les travaux à exécuter à chaque étape du processus.`

On distingue trois catégories principales d’opérateurs:
- `les opérateurs d’actions` exécutent une fonction.
- `les opérateurs de transfert` quant à eux permettent de transférer des données depuis une source vers une destination.
- `les Sensors (capteurs)` permettent d’attendre qu’une condition soit vérifiée. 

### Les Hooks
Sur Airflow, les `Hooks` permettent l’interface avec des systèmes tiers. Ils permettent de se connecter à des APIs et des bases de données externes 

### Les plugins
Les `plugins` Airflow peuvent être décrits comme une combinaison entre les Hooks et les Opérateurs. Ils sont utilisés pour accomplir certaines tâches spécifiques impliquant une application externe.

### Les connexions
Les `connexions`  permettent à Airflow de stocker des informations, permettant de se connecter à des systèmes externes comme des identifiants ou des tokens d’API.


### A retenir
- un DAG est une collection de tasks
- un Task est une collection d'operators

## Task Lifecycle
Une tâche passe par différentes étapes du début à la fin. Dans l'interface utilisateur d'Airflow (vues graphique et arborescence), ces étapes sont affichées par une couleur représentant chaque étape :

![](img/Screenshot%202024-09-15%20at%2017.34.51.png)


- No status (le scheduler a créé une instance de tâche vide)

- Scheduled (le scheduler a déterminé que l'instance de tâche doit être exécutée)

- Queued (le scheduler a envoyé la tâche à l'exécuteur pour qu'elle s'exécute dans la file d'attente)

- Running (worker (le travailleur a récupéré une tâche et l'exécute maintenant)

- Success (tâche terminée)
## Airflow Dag with BashOperator
Pour ne plus avoir d'exeemple de dags sur airflow merci modifier le variable `AIRFLOW__CORE__LOAD_EXAMPLES` à `false` dans le fichier docker-compose comme suit:

```yaml
    ...
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    ...
```
## Reference

[Airflow Concepts](https://airflow.apache.org/docs/apache-airflow/2.0.1/concepts.html)